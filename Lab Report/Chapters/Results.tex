\section{Results}
\label{results}
During our experiments, we observed that there is a huge difference in the policy behavior between the simulation and physical system. Often there will be policies running flawlessly in simulation but are unable to solve the problem even closely in reality. Further, NPG and NES behave quite differently during their training. Thus, in order to present and discuss our findings, we will split this section into two parts, Simulation and Physical System.

\subsection{Simulation}
\label{sim}

\begin{figure*}
\centering
\includegraphics[scale=.5]{plots/qube_compare.pdf}
\caption{Comparison of the training process with different hyperparameters. The plots show the huge impact even a single hyperparameter can have on the training process.}
\label{fig:NPG_training}
\end{figure*}

Training a policy with the NPG turned out to be quite difficult. The biggest hurdle is finding suitable hyperparameters. \autoref{fig:NPG_training} shows the training process of the NPG on the left and NES on the right with two different sets of hyperparameters each. For the NPG we chose to change the discount factor $\gamma$, however, this is only one of many hyperparameters that can be changed in order to solve the optimization problem. \newpage
The most important parameters are:
\begin{itemize}
  \item Trust Region $\delta$: \\
  $\delta$ regulates the step size of the policy update which can lead to faster convergence. However, this can also result in worse performance because the algorithm may not converge to a global optimum and instead jumps among suboptimal solutions. We also observed that for varying $\delta$ we can get large jumps in performance during the training process as evident in the top left of \autoref{fig:NPG_final} \smallskip
  \item Discount Factor $\gamma$: \\
  $\gamma$ determines the focus to short or long term rewards. Increasing the value also increases the importance of long term reward and vice versa \cite{Schulman2018}. \smallskip
  \item $\lambda$: \\
  $\lambda$ is used to realize a tradeoff between bias and variance in the advantage estimation \cite{Schulman2018}. \smallskip
  \item Number of rollouts $n$: \\
  A higher number of rollouts per episode decreases the variance between episodes as the distribution over possible starting positions is averaged more realistically.\smallskip
  \item Policy $\pi$: \\
  The policy maps states to actions. As such depending on the tasks to solve different policy classes can be applied \cite{Rajeswaran2017}. For our implementation, we only use a neural network based gaussian policy, which not only the NPG, as implemented at this point, is efficient for, but also makes feature construction unnecessary. For our project, we decided to use the Adam optimizer as well as the $\tanh$ activation function, however, the activation function can also be changed if needed. The main difficulty is to find the right network size. \smallskip
  \item Baseline $b$: \\
  The baseline estimates the state-value function. Possibly every regression algorithm can be used to realize the estimation provided suitable features are used. To simplify the implementation, we decided to represent the baseline with a neural network as well. In order to increase its performance, we added batch normalization. As such the baseline itself has multiple parameters that can be changed. The network size and number of epochs are our main concern. The batch size is set to 64 as default but can be adjusted as well as the learning rate which is set to $10^{-3}$. The optimizer and activation is the same as in the policy network. Despite fixing the optimizer, we noticed during our experiments that the optimizer has a huge impact. Adam behaves very differently to e.g. the SGD.
\end{itemize}
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{plots/learned_benchmarked_NPG.pdf}
\caption{This figure shows the training process of the NPG as well as a benchmark with the final policy on each of the three tasks described in \autoref{intro}. "Qube" is referring to the Furuta Pendulum.}
\label{fig:NPG_final}
\end{figure*}

\autoref{fig:NPG_final} shows our final results on the given tasks using the NPG. We recognized that the most important parameters are $\gamma$ and $\lambda$ of the advantage function. We have had a lot of trouble finding suitable parameters for the given tasks and, unfortunately, have not managed to solve the Cartpole swing up. \\
Another important insight is provided by the benchmark plots, which are shown in the lower row of \autoref{fig:NPG_final}. Each plot illustrates ten of the 100 runs of the benchmark test. It is important, depending on the environment, to perform a representative amount of runs, because the starting position can vary, which might lead to false positive results as pictured in the bottom left plot in \autoref{fig:NPG_final}. During one of the runs the simulation was terminated early because the robot hit the restriction in the movement space and did not manage to perform the swing up. To ensure the best performance the exploration was set to zero for the benchmarks. \\
In the produced experiments we have even encountered policies whose exploration converged to almost zero and thus the total reward of the last training episodes was close to the benchmark performance. This is the desired behavior of gaussian policies, which should converge to an optimal policy without further exploration after sufficient training with suitable hyperparameters (see \autoref{fig:NES_final} Double Pendulum). \\

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{plots/learned_benchmarked_NES.pdf}
\caption{Final results for given tasks using SNES}
\label{fig:NES_final}
\end{figure*}

As already mentioned in \autoref{intro} with the Canonical NES we had similar problems as with the first version of the NPG. Inverting the FIM often resulted in singular matrices, but in addition, both were very slow. In the case of the NES, we decided to switch to the SNES, which turned out to be a huge success. \autoref{fig:NES_final} illustrates our final results with the SNES on all tasks. We managed to solve all platforms due to the fact that the SNES compared to the NPG has fewer hyperparameters that need to be adjusted. The only ones being:
\begin{itemize}
  \item Population size $\lambda$: \\
  The population size $\lambda$ defines the number of samples which are drawn from the search distribution for each episode. For all our solutions we used the calculation for $\lambda$ suggested by \cite{Wierstra14}. \smallskip
  \item Number of rollouts $n$: \\
  In the case of the NES, this parameter does not define the number of rollouts per episode but rather the rollouts performed for each policy evaluation. Thus, for each episode $\lambda \cdot n$ simulations are executed while resetting the environment seed between each sample to a random value chosen at the beginning of each episode. Increasing the number of rollouts decreases the variance during rollouts similar to the NPG. Our experiments have shown that for most platforms and tasks $n=2$ turned out to be enough to find good solutions. To solve the Furuta Pendulum, however, we had to use $n=15$ as it was much more unstable than the Cartpole These settings have also been used to produce the plots in \autoref{fig:NES_final}. \smallskip
  \item Policy $\pi$: \\
  Similar to the NPG the policy can be chosen arbitrarily depending on tasks and setup. We decided to use the greedy version of the policy we used for the NPG as the NES already incorporates exploration in the search distribution over the policy parameters. Additionally, we avoid the same problems using a neural network as described for the NPG. As such we have the same adjustable parameters for the NN, which however turned out not to be as impactful as in the NPG. For all tasks, we used a neural network with a single layer and 10 nodes and the default $\tanh$ activation function. \smallskip
  \item Fitness function $f(z)$: \\
  The fitness function is crucial to evaluate the performance of the sample policies $\pi_z$. There a many possible options to choose from. We decided to go with one of the easiest by using $f(z) = \sum_{k=1}^{n} \sum_{i=1}^{T} r_{k,t}$.
\end{itemize}

In the end the SNES was not only easier to implement but also much easier to handle and set up. It has a lot less crucial hyperparameters that need to be adjusted and in addition, it is also much more invariant to small changes in the hyperparameters. We have observed that even though the SNES takes much longer than the NPG to reduce its exploration to near zero it manages to converge much faster to a policy of high quality.

\subsection{Physical System}
\label{phys}
\vspace{-.8cm}
\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{plots/nes_on_rr_master.pdf}
\caption{Trained policy using NES in simulation and applied to physical Qube}
\label{fig:NES_sim_to_physqube}
\end{figure}

Learning a policy in simulations is only the first step. In the end, we need to control a real physical system. Anyhow, applying a policy trained in simulation to a physical system can lead to tremendous drops in performance. \autoref{fig:NES_sim_to_physqube} pictures this drop in performance really well. As can be seen, the benchmark ran flawlessly in the simulation while crashing spectacularly on the physical system. We got a similar result for the NPG on the Furuta Pendulum. One reason we noticed was that the state space was slightly smaller in reality due to an obstacle mounted on the Qube to prevent damage when using suboptimal controllers. In order to apply policies without having this issue, we decided to clip the state space of the simulation to $[-\pi, \pi]$ and retrained the policies with both algorithms.

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{plots/npg_on_rr_master.pdf}
\caption{Policy performance using NPG and clipped state space}
\label{fig:NPG_sim_to_physqube}
\end{figure}

Applying the new policy learned with the NPG worked surprisingly well. \autoref{fig:NPG_sim_to_physqube} displays the benchmark results for our policy trained in the new state space. It is evident that the drops in performance were less sever than before. The NPG even managed to perform the swing up and stabilization of the Furuta Pendulum in all benchmark runs. However, when applying the NES we recognized that this is not the case for all policies as the NES policy did not perform a single swing up. Instead, with this particular NPG policy, we got lucky as it was not as efficient during the swing up period as the others. In \autoref{fig:NES_sim_to_physqube} and \ref{fig:NPG_sim_to_physqube} we notice the difference in the swing up of the simulation. The new NPG policy took longer to perform the swing up and used two instead of one swing before getting into an upright position while the new NES policy performed similarly to before as in \autoref{fig:NES_sim_to_physqube}. On that basis we conclude that most policies are trained to a point were they perform unrealistically good and thus are overfitted to the simulation.
So in order to train a good controller for real systems we can either train a policy in simulation and tune it on the physical system or train on the physical system from the start. Due to limited time we only used the first method.
Though while tuning the new NES policy on the real system we encountered a huge problem. Every few runs the system crashed without stopping the training process. Instead, when crashed, each run ended after few steps with almost no reward which made it nearly impossible to improve the policy. Due to this fact, we decided to try solving the real Cartpole swing up with the NES. Similar to the Qube the NES performed nearly perfectly in simulation and its performance dropped on the real Cartpole. \autoref{fig:NES_sim_to_physcart} shows the proccess of applying the NES to the Carpole in the first row while the second illustrates the improvement of the policy after 30 and 60 episodes. To prevent damage to the system we have clipped the actions to $[-6, 6]$. We also managed to solve the Ball-Balancer-v0 and Levitation-v0 with the same parameters using the NES, but did not include the results due to a lack of space.
\begin{figure}
\centering
\includegraphics[scale=.5]{plots/training_on_rr.pdf}
\caption{Tuning of new NES policy on real Cartpole}
\label{fig:NES_sim_to_physcart}
\end{figure}
